# LSTM

## 用于情感分类
本地训练环境：4核4G内存（CPU）

### 第一次训练

数据量：

```
wc -l /var/www/data/sentiments/* 
18576 /var/www/data/sentiments/neg.txt
16548 /var/www/data/sentiments/pos.txt
1426 /var/www/data/sentiments/stopwords.txt
```

训练及预测效果：

```
单个样本最大长度：  1795
去重后的有效词的数量：  42606

Epoch 1/10
28099/28099 [==============================] - 1872s - loss: 0.3869 - acc: 0.8324 - val_loss: 0.2604 - val_acc: 0.8979
Epoch 2/10
28099/28099 [==============================] - 1792s - loss: 0.1949 - acc: 0.9283 - val_loss: 0.2627 - val_acc: 0.9153
Epoch 3/10
28099/28099 [==============================] - 1751s - loss: 0.1437 - acc: 0.9488 - val_loss: 0.2036 - val_acc: 0.9315
Epoch 4/10
28099/28099 [==============================] - 1724s - loss: 0.0898 - acc: 0.9685 - val_loss: 0.1994 - val_acc: 0.9372
Epoch 5/10
28099/28099 [==============================] - 1726s - loss: 0.0631 - acc: 0.9780 - val_loss: 0.2104 - val_acc: 0.9426
Epoch 6/10
28099/28099 [==============================] - 1727s - loss: 0.0507 - acc: 0.9822 - val_loss: 0.2258 - val_acc: 0.9422
Epoch 7/10
28099/28099 [==============================] - 1729s - loss: 0.0426 - acc: 0.9849 - val_loss: 0.2216 - val_acc: 0.9441
Epoch 8/10
28099/28099 [==============================] - 1724s - loss: 0.0419 - acc: 0.9848 - val_loss: 0.2558 - val_acc: 0.9443
Epoch 9/10
28099/28099 [==============================] - 1726s - loss: 0.0278 - acc: 0.9890 - val_loss: 0.2829 - val_acc: 0.9469
Epoch 10/10
28099/28099 [==============================] - 1727s - loss: 0.0247 - acc: 0.9908 - val_loss: 0.2645 - val_acc: 0.9498
7025/7025 [==============================] - 125s     

Test score: 0.265, accuracy: 0.950
```

在训练集上准确率达到99%，在测试集上准确率达到95%，比普通的方法确实好了许多。

### 第二次训练：分词之后增加黑名单过滤

```
单个样本最大长度：  893
去重后的有效词的数量：  42606
```

每个epoch的耗时下降到900秒左右，训练集准确度0.9898，测试集准确度0.941。

### 第三次训练：去掉低频词（词频为1的词）
修改为epoch num=5

```
单个样本最大长度：  893
去重后的有效词的数量：  42606
样本有效长度分布： Counter({0: 34682, 1: 384, 2: 38, 3: 11, 4: 5, 5: 2, 7: 1, 8: 1})
只保留词频大于1的
单个样本最大长度：  777
去重后的有效词的数量：  38605
样本有效长度分布： Counter({0: 34696, 1: 379, 2: 32, 3: 12, 4: 2, 6: 1, 7: 1, 5: 1})
```

样本的最大长度减小13%，总有效词数量减少10%，每个epoch的耗时下降到800秒左右，训练集准确度0.9812，测试集准确度0.937

问题：从debug输出的信息看，样本的有效长度受个别文本影响比较大

### 第四次训练：样本有效长度只覆盖95%的文本

```
单个样本最大长度：  893
去重后的有效词的数量：  42606
样本有效长度分布： Counter({0: 34682, 1: 384, 2: 38, 3: 11, 4: 5, 5: 2, 7: 1, 8: 1})
只保留词频大于1的
单个样本最大长度：  777
去重后的有效词的数量：  38605
样本有效长度分布： Counter({1: 11222, 0: 9208, 2: 5246, 3: 2636, 5: 2471, 4: 2196, 6: 1067, 7: 373, 8: 179, 10: 106, 9: 98, 11: 91, 12: 56, 13: 32, 14: 29, 19: 16, 15: 15, 16: 14, 18: 12, 20: 9, 17: 8, 23: 6, 21: 5, 22: 4, 24: 3, 28: 3, 32: 3, 33: 2, 35: 2, 30: 2, 37: 1, 34: 1, 65: 1, 25: 1, 47: 1, 42: 1, 36: 1, 29: 1, 77: 1, 56: 1})
MAX_FEATURES:  38605
MAX_SENTENCE_LENGTH:  60
```

每个epoch耗时130秒，训练集准确度0.9803, 测试集准确度0.941


### 第五次训练：样本有效长度覆盖98%的文本




